{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-ready",
   "metadata": {
    "id": "latest-ready"
   },
   "outputs": [],
   "source": [
    "#!pip install -U torch torchvision opencv-python-headless\n",
    "!pip install openmim\n",
    "!mim install mmcv-full\n",
    "!mim install mmdet\n",
    "!mim install mmpose\n",
    "#!pip install anime-face-detector\n",
    "!git clone https://github.com/hysts/anime-face-detector\n",
    "\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OSpMJUNxiYx8",
   "metadata": {
    "id": "OSpMJUNxiYx8"
   },
   "source": [
    "If you encounter the following error in Colab, you can restart the runtime to execute the following cells correctly.\n",
    "\n",
    "```\n",
    "xtcocotools/_mask.pyx in init xtcocotools._mask()\n",
    "ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8DJZMVHhLIfI",
   "metadata": {
    "id": "8DJZMVHhLIfI"
   },
   "outputs": [],
   "source": [
    "%cd anime-face-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LO6BhPqtZSbi",
   "metadata": {
    "cellView": "form",
    "id": "LO6BhPqtZSbi"
   },
   "outputs": [],
   "source": [
    "#@title visualize function\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import anime_face_detector\n",
    "\n",
    "\n",
    "def detect_vis(img, face_score_threshold: float,\n",
    "               landmark_score_threshold: float,\n",
    "               detector: anime_face_detector.LandmarkDetector):\n",
    "    preds = detector(img)\n",
    "    res = img.copy()\n",
    "\n",
    "    for pred in preds:\n",
    "        box = pred['bbox']\n",
    "        box, score = box[:4], box[4]\n",
    "        if score < face_score_threshold:\n",
    "            continue\n",
    "        box = np.round(box).astype(int)\n",
    "\n",
    "        lt = max(2, int(3 * (box[2:] - box[:2]).max() / 256))\n",
    "\n",
    "        cv2.rectangle(res, tuple(box[:2]), tuple(box[2:]), (0, 255, 0), lt)\n",
    "\n",
    "        pred_pts = pred['keypoints']\n",
    "        for *pt, score in pred_pts:\n",
    "            if score < landmark_score_threshold:\n",
    "                color = (0, 255, 255)\n",
    "            else:\n",
    "                color = (0, 0, 255)\n",
    "            pt = np.round(pt).astype(int)\n",
    "            cv2.circle(res, tuple(pt), lt, color, cv2.FILLED)\n",
    "    #res = cv2.cvtColor(res, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #image_pil = PIL.Image.fromarray(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "# https://github.com/hysts/anime-face-detector/blob/main/assets/landmarks.jpg\n",
    "FACE_BOTTOM_OUTLINE = list(range(0, 5))\n",
    "MOUTH_OUTLINE = list(range(24, 28))\n",
    "LEFT_EYEBROW = list(range(5, 8))\n",
    "RIGHT_EYEBROW = list(range(8, 11))\n",
    "LEFT_EYE_TOP = list(range(11, 14))\n",
    "RIGHT_EYE_TOP = list(range(17, 20))\n",
    "LEFT_EYE_BOTTOM = list(range(14, 17))\n",
    "RIGHT_EYE_BOTTOM = list(range(20, 23))\n",
    "NOSE = list([23])\n",
    "FACE_ALL_LIST = [\n",
    "    FACE_BOTTOM_OUTLINE, MOUTH_OUTLINE, LEFT_EYEBROW, RIGHT_EYEBROW,\n",
    "    LEFT_EYE_TOP, RIGHT_EYE_TOP, LEFT_EYE_BOTTOM, RIGHT_EYE_BOTTOM, NOSE\n",
    "]\n",
    "LEFT_EYE_LIST = [LEFT_EYE_TOP, LEFT_EYE_BOTTOM]\n",
    "RIGHT_EYE_LIST = [RIGHT_EYE_TOP, RIGHT_EYE_BOTTOM]\n",
    "FACE_OUTLINE_LIST = [FACE_BOTTOM_OUTLINE, LEFT_EYEBROW, RIGHT_EYEBROW]\n",
    "MOUTH_OUTLINE_LIST = [MOUTH_OUTLINE]\n",
    "NOSE_LIST = [NOSE]\n",
    "\n",
    "\n",
    "def detect_vis2(img, face_score_threshold: float,\n",
    "                landmark_score_threshold: float,\n",
    "                detector: anime_face_detector.LandmarkDetector):\n",
    "    preds = detector(img)\n",
    "    res = img.copy()\n",
    "\n",
    "    for pred in preds:\n",
    "        # vis box\n",
    "        box = pred['bbox']\n",
    "        box, score = box[:4], box[4]\n",
    "        box = np.round(box).astype(int)\n",
    "        lt = max(2, int(3 * (box[2:] - box[:2]).max() / 256))  #line_thickness\n",
    "\n",
    "        cv2.rectangle(res, tuple(box[:2]), tuple(box[2:]), (0, 255, 0), lt)\n",
    "        cv2.putText(res,\n",
    "                    str(round(score * 100, 2)) + '%', (box[0], box[1] - 2),\n",
    "                    0,\n",
    "                    lt / 2, [225, 255, 255],\n",
    "                    thickness=max(lt, 1),\n",
    "                    lineType=cv2.LINE_AA)\n",
    "\n",
    "        # vis landmark\n",
    "        pred_pts = pred['keypoints']\n",
    "        th_pred_pts = []\n",
    "        # points num = 28\n",
    "        for i in range(28):\n",
    "            *pt, score = pred_pts[i]\n",
    "            pt = tuple(np.round(pt).astype(int))\n",
    "            if score < landmark_score_threshold:\n",
    "                color = (0, 255, 255)\n",
    "                th_pred_pts.append(None)\n",
    "            else:\n",
    "                color = (0, 0, 255)\n",
    "                th_pred_pts.append(np.array(pt, np.int32))\n",
    "            cv2.circle(res, pt, lt, color, cv2.FILLED)\n",
    "\n",
    "        # for each parts\n",
    "        #print(th_pred_pts)\n",
    "        for points in FACE_ALL_LIST:\n",
    "            #print(points)\n",
    "            pts = [th_pred_pts[_] for _ in points]\n",
    "            #print(pts)\n",
    "\n",
    "            # pass none group\n",
    "            hasNone = False\n",
    "            for _ in pts:\n",
    "                if isinstance(_, type(None)):\n",
    "                    hasNone = True\n",
    "                    break\n",
    "            if hasNone:\n",
    "                break\n",
    "\n",
    "            closed = False\n",
    "            if points in FACE_OUTLINE_LIST:\n",
    "                color = (0, 170, 255)\n",
    "            elif points in NOSE_LIST:\n",
    "                color = (255, 30, 30)\n",
    "            elif points in LEFT_EYE_LIST:\n",
    "                color = (50, 220, 255)\n",
    "            elif points in RIGHT_EYE_LIST:\n",
    "                color = (50, 220, 255)\n",
    "            elif points in MOUTH_OUTLINE_LIST:\n",
    "                color = (255, 30, 30)\n",
    "                closed = True\n",
    "            else:\n",
    "                raise (Exception(f'unknow points {points}'))\n",
    "            cv2.polylines(res, np.array([pts], np.int32), closed, color, lt)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zkcySBnkZsUZ",
   "metadata": {
    "cellView": "form",
    "id": "zkcySBnkZsUZ"
   },
   "outputs": [],
   "source": [
    "#@title ArgumentParser\n",
    "\n",
    "device = 'cpu'  #@param ['cuda:0', 'cpu']\n",
    "model = 'yolov3'  #@param ['yolov3', 'faster-rcnn']\n",
    "visualize_func = 'detect_vis2'  #@param ['detect_vis2', 'detect_vis']\n",
    "\n",
    "detector = anime_face_detector.create_detector('yolov3', device=device)\n",
    "detect = detect_vis2 if visualize_func == 'detect_vis2' else detect_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EA46D11letIK",
   "metadata": {
    "id": "EA46D11letIK"
   },
   "source": [
    "# image test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jvfoo0jV4Klt",
   "metadata": {
    "id": "Jvfoo0jV4Klt"
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/hysts/anime-face-detector/main/assets/input.jpg -O input.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yTUyNEKEZuVh",
   "metadata": {
    "id": "yTUyNEKEZuVh"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.imread('input.jpg')\n",
    "res = detect(image,\n",
    "             face_score_threshold=0.5,\n",
    "             landmark_score_threshold=0.3,\n",
    "             detector=detector)\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "plt.imshow(res[:, :, ::-1])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IVE9R_OgexgZ",
   "metadata": {
    "id": "IVE9R_OgexgZ"
   },
   "source": [
    "# video test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geHrwtbxkjdk",
   "metadata": {
    "cellView": "form",
    "id": "geHrwtbxkjdk"
   },
   "outputs": [],
   "source": [
    "#@title detect_from_video function\n",
    "# from https://github.com/aim-uofa/AdelaiDet/blob/master/demo/predictor.py\n",
    "def detect_from_video(video,\n",
    "                      face_score_threshold,\n",
    "                      landmark_score_threshold,\n",
    "                      detector=detector):\n",
    "    def process_predictions(frame):\n",
    "        return detect(frame,\n",
    "                      face_score_threshold,\n",
    "                      landmark_score_threshold,\n",
    "                      detector=detector)\n",
    "\n",
    "    def _frame_from_video(video):\n",
    "        while video.isOpened():\n",
    "            success, frame = video.read()\n",
    "            if success:\n",
    "                yield frame\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    frame_gen = _frame_from_video(video)\n",
    "    for frame in frame_gen:\n",
    "        yield process_predictions(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-gauge",
   "metadata": {
    "id": "n67jMIyQujHe"
   },
   "outputs": [],
   "source": [
    "#https://www.sakugabooru.com/post/show/43401\n",
    "!wget -q https://www.sakugabooru.com/data/f47f699b9c5afc5a849be4b974f40975.mp4 -O input_vid.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n67jMIyQujHe",
   "metadata": {
    "id": "n67jMIyQujHe"
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# skip frame\n",
    "speedx = 2\n",
    "clip = VideoFileClip('input_vid.mp4').subfx(lambda c: c.speedx(speedx))\n",
    "clip.write_videofile('input_vid_clip.mp4')\n",
    "clip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xAumCV4WI7c8",
   "metadata": {
    "id": "xAumCV4WI7c8"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "video = cv2.VideoCapture('input_vid_clip.mp4')\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frames_per_second = video.get(cv2.CAP_PROP_FPS) // speedx\n",
    "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "output_file = cv2.VideoWriter(\n",
    "    filename='/content/anime-face-detector/output_vid.mp4',\n",
    "    # some installation of opencv may not support x264 (due to its license),\n",
    "    # you can try other format (e.g. MPEG)\n",
    "    fourcc=cv2.VideoWriter_fourcc(*'MPEG'),\n",
    "    fps=float(frames_per_second),\n",
    "    frameSize=(width, height),\n",
    "    isColor=True,\n",
    ")\n",
    "\n",
    "# colabCPU 3.27s/it\n",
    "for vis_frame in tqdm(detect_from_video(video,\n",
    "                                        face_score_threshold=0.5,\n",
    "                                        landmark_score_threshold=0.3,\n",
    "                                        detector=detector),\n",
    "                      total=num_frames):\n",
    "    output_file.write(vis_frame)\n",
    "\n",
    "video.release()\n",
    "output_file.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZmB48HNW1Xvd",
   "metadata": {
    "id": "ZmB48HNW1Xvd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hysts/anime-face-detector.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
