import glob
import os
import shutil
from utils import files_in_dir, initialize_training_set
from utils_ML import train_image_type, classify_image_type, train_face_recognition_tf, classify_all_characters_tf, load_existing_model, classify_all_images_tf

def filter_dataset_by_imagetype(dataset_dir_in="datasets_base",dataset_dir_out="datasets_anime",rejected_dir_top="Images/rejected_images",model=None):
    '''
    INPUTS
    All variables point to the top level directory, with images stored in their repsective class folders.
    dataset_dir_in - initial dataset, in this case generated by downloading images from google images, which isn't very precise
    dataset_dir_out - dataset for accepted images that are determined to match the artstyle of the anime we want
    rejected_dir_top - dataset to store images that are filtered out
    '''
    dirs = glob.glob(os.path.join(dataset_dir_in,'*'))
    for dir in dirs:
        images_list = files_in_dir(dir)
        accepted_dir = os.path.join(dataset_dir_out,os.path.basename(dir))
        os.makedirs(accepted_dir,exist_ok=True)
        rejected_dir = os.path.join(rejected_dir_top,os.path.basename(dir))
        os.makedirs(rejected_dir,exist_ok=True)
        for file in images_list:
            predicted_class_label, model = classify_image_type(file,model=model)
            if predicted_class_label == "this_anime":
                shutil.copy(file,os.path.join(accepted_dir,os.path.basename(file)))
            else:
                name, ext = os.path.splitext(os.path.basename(file))
                out_name = os.path.join(rejected_dir,name+'-'+predicted_class_label+ext)
                shutil.copy(file,out_name)



if __name__ == "__main__":
    '''
    # Assume we are starting with a dataset of downloaded images using get_character_images() from utils.py
    model = train_image_type() # Train dataset to identify faces from this anime vs other anime or from real life
    filter_dataset_by_imagetype("datasets_base","datasets_anime",model=model) # Use this model to filter downloaded images into datasets_anime
    # Create initial training set using just images from myanimelist.
    initialize_training_set(out_dir='Images/myanimelist-training')
    # Copy this one-image-per-class set to to first iteration training set.
    shutil.copytree('Images/myanimelist-training','datasets_iterative0',dirs_exist_ok=True)
    # Train initial set, saving but not returning model so we don't have to run all of this at once
    train_face_recognition_tf(training_dir='datasets_iterative0',out_name='models/FRmodel0.h5',num_augmented_images=150)
    # Create next dataset using this model
    shutil.copytree('Images/myanimelist-training','datasets_iterative1',dirs_exist_ok=True)
    classify_all_characters_tf('datasets_anime','datasets_iterative1')
    # Train next set, saving but not returning model so we don't have to run all of this at once
    train_face_recognition_tf(training_dir='datasets_iterative1',out_name='models/FRmodel1.h5',num_augmented_images=50)
    # Create next dataset using this model
    shutil.copytree('Images/myanimelist-training','datasets_iterative2',dirs_exist_ok=True)
    classify_all_characters_tf('datasets_anime','datasets_iterative2',model_name='models/FRmodel1.h5')
    # Load model back in to speed up training. In this run we will bump up regularization to clean out bad images that only exist due to overfitting.
    model = load_existing_model('models/FRmodel1.h5')
    # Train next set, saving but not returning model so we don't have to run all of this at once.
    train_face_recognition_tf(training_dir='datasets_iterative2',out_name='models/FRmodel2.h5',num_augmented_images=40,reg=5,model=model)
    # Create next dataset using this model.
    shutil.copytree('Images/myanimelist-training','datasets_iterative3',dirs_exist_ok=True)
    # We set best_only to true so that if a character is similar but in the wrong set, it will be passed over as long as its true class is more likely.
    classify_all_characters_tf('datasets_anime','datasets_iterative3',model_name='models/FRmodel2.h5',best_only=True)
    '''
    # Load model back in to speed up training. In this run we will bump up regularization to clean out bad images that only exist due to overfitting.
    model = load_existing_model('models/FRmodel2.h5')
    # Train next set, saving but not returning model so we don't have to run all of this at once.
    train_face_recognition_tf(training_dir='datasets_iterative3',out_name='models/FRmodel3.h5',num_augmented_images=25,reg=50,model=model)
    # Create next dataset using this model.
    shutil.copytree('Images/myanimelist-training','datasets_iterative4',dirs_exist_ok=True)
    # We set best_only to true so that if a chracter is similar but in the wrong set, it will be passed over as long as its true class is more likely.
    classify_all_images_tf('datasets_anime','datasets_iterative4',model_name='models/FRmodel3.h5',a_thresh=.6)






